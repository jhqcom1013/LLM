{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My API Key: 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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# # Access the API key\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# print(f\"My API Key: {api_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 68\u001b[0m\n\u001b[0;32m     64\u001b[0m         bot_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat_func(history)\n\u001b[0;32m     66\u001b[0m         history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: bot_response})\n\u001b[1;32m---> 68\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun(continous_chat())\n",
      "File \u001b[1;32mc:\\Users\\wcsnj\\anaconda3\\envs\\chatbot_env\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Construct the system prompt\n",
    "system_prompt_template = \"\"\"You are Bobby, a virtual assistant create by Huajun. Today is {today}. You provide responses to questions that are clear, straightforward, and factually accurate, without speculation or falsehood. Given the following context, please answer each question truthfully to the best of your abilities based on the provided information. Answer each question with a brief summary followed by several bullet points. \n",
    "\n",
    "\n",
    "Example:\n",
    "Summary of answer\n",
    "- bullet point 1\n",
    "- bullet point 2\n",
    "...\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "with open(\"news_result.txt\") as in_file:\n",
    "    context_content = in_file.read()\n",
    "\n",
    "system_prompt = system_prompt_template.format(\n",
    "    context=context_content, \n",
    "    today=datetime.today().strftime('%Y-%m-%d')\n",
    ")\n",
    "\n",
    "async def chat_func(history):\n",
    "\n",
    "    result = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt}] + history,\n",
    "        max_tokens=256,\n",
    "        temperature=0.5,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    buffer = \"\"\n",
    "    async for r in result:\n",
    "        next_token = r.choices[0].delta.content\n",
    "        if next_token:\n",
    "            print(next_token, flush=True, end=\"\")\n",
    "            buffer += next_token\n",
    "\n",
    "    print(\"\\n\", flush=True)\n",
    "\n",
    "    return buffer\n",
    "\n",
    "async def continous_chat():\n",
    "    history = []\n",
    "\n",
    "    # Loop to receive user input continously\n",
    "    while(True):\n",
    "        user_input = input(\"> \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # notice every time we call the chat function\n",
    "        # we pass all the history to the API\n",
    "        bot_response = await chat_func(history)\n",
    "\n",
    "        history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "\n",
    "asyncio.run(continous_chat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
